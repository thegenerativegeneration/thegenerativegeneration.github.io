<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://thegenerativegeneration.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://thegenerativegeneration.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-27T18:03:49+00:00</updated><id>https://thegenerativegeneration.github.io/feed.xml</id><title type="html">Philipp Haslbauer</title><subtitle>Software developer, researcher, and artist based in Lucerne, Switzerland. </subtitle><entry><title type="html">Easiest Way To Use 3D Gaussians Splatting</title><link href="https://thegenerativegeneration.github.io/blog/2024/easy-3dgs/" rel="alternate" type="text/html" title="Easiest Way To Use 3D Gaussians Splatting"/><published>2024-01-06T09:08:44+00:00</published><updated>2024-01-06T09:08:44+00:00</updated><id>https://thegenerativegeneration.github.io/blog/2024/easy-3dgs</id><content type="html" xml:base="https://thegenerativegeneration.github.io/blog/2024/easy-3dgs/"><![CDATA[<p>I am currently preparing a course on applied novel 3D graphics for media professionals and artists who want to learn how to use 3D reconstructions, AI-created assets and augmented reality in their work.</p> <h2 id="3d-gaussian-splatting-and-ar">3D Gaussian Splatting and AR</h2> <p>To this end, I am on the lookout for the easiest possible way to create experiences from 3D reconstructions made with Gaussian Splatting.</p> <p>One of the possible options is <strong>Spline</strong> which works without any code. Spline sadly does not have any augmented reality capabilities.</p> <p>I have created a sample experience below.</p> <details> <summary>View Spline Experience</summary> <script type="module" src="https://unpkg.com/@splinetool/viewer@1.9.52/build/spline-viewer.js"></script> <spline-viewer url="https://prod.spline.design/15bXeAhQL-MA-eLO/scene.splinecode"></spline-viewer> </details> <p>Another up-and-coming options is <strong>Niantic Studio</strong> with <strong>8th Wall</strong>. It supports AR and VR. As far as I have seen some coding may be required which makes it a suboptimal option for the course.</p> <details> <summary>View 8th Wall Experience</summary> <iframe src="https://bananarama.8thwall.app/gauss-test/" allow="camera;gyroscope;accelerometer;magnetometer;xr-spatial-tracking;microphone;" style="width: 100%; height: 600px; border: none;"></iframe> </details> <p>Both 8th Wall and Spline have a free tier, support Gaussian Splatting and allow sharing of created experiences without self-hosting.</p> <p>Meta’s <strong>Spark AR</strong> which I originally had in mind was sadly shut down at the start of 2025. It was a great tool for creating AR experiences within Meta’s products, although it did not yet support Gaussian Splatting.</p> <h2 id="generative-3d-models">Generative 3D Models</h2> <p><em>To be continued…</em></p>]]></content><author><name></name></author><category term="3dgs"/><category term="3d-graphics"/><category term="augmented-reality"/><summary type="html"><![CDATA[How can I teach people who have never worked with 3D graphics to use 3D Gaussian Splatting?]]></summary></entry><entry><title type="html">Control - How to Steer Diffusion Models</title><link href="https://thegenerativegeneration.github.io/blog/2023/controlling-diffusion/" rel="alternate" type="text/html" title="Control - How to Steer Diffusion Models"/><published>2023-03-05T15:08:44+00:00</published><updated>2023-03-05T15:08:44+00:00</updated><id>https://thegenerativegeneration.github.io/blog/2023/controlling-diffusion</id><content type="html" xml:base="https://thegenerativegeneration.github.io/blog/2023/controlling-diffusion/"><![CDATA[<p>On this page, I have compiled several methods to steer the diffusion process. If not otherwise stated, I present the methods as they are used with Stable Diffusion.</p> <p>Although the mechanisms presented are used in 2D image generation, I assume that they can be used in the generation of other modalities.</p> <p>I will try to keep this article up to date. If you have any suggestions, please let me know.</p> <h1 id="finetune">Finetune</h1> <h2 id="cross-attention">Cross Attention</h2> <p>In Stable Diffusion (and Imagen/Dalle 2) text prompts are used to condition the image generation. The text prompts are encoded by a text model. The encoding is passed into the diffusion model to a cross-attention layer. The cross-attention layer can also be used to condition on different information. However, adding new conditioning requires retraining.</p> <p>This is probably not great when you want to combine multiple conditionings as each would likely require its own cross-attention layers.</p> <ul> <li>increases model size</li> <li>adds cross-attention layers</li> <li>requires retraining or finetuning</li> </ul> <h2 id="concatenation">Concatenation</h2> <p>If the conditioning you want to use has the same size as the diffusion model input, you can use concatenation. If it doesn’t, you can still embed it into this shape.</p> <p>For example, you can train an upsampling model by concatenating (in the channel dimension) the noisy input and a lower-resolution input that is bilinearly upsampled to the input size. For this specific use case, the input size of the model increases from <code class="language-plaintext highlighter-rouge">BxCxHxW</code> to <code class="language-plaintext highlighter-rouge">Bx2*CxHxW</code> (“Guided-Diffusion Paper”).</p> <p>Some video diffusion papers applied this method to interpolate frames in the frame/time dimension.</p> <ul> <li>increases input size in the channel dimension</li> <li>requires retraining or finetuning</li> <li>can be an alternative to cross-attention</li> </ul> <h1 id="train-a-new-model">Train a New Model</h1> <h2 id="controlnet"><a href="https://github.com/lllyasviel/ControlNet">ControlNet</a></h2> <p>ControlNet introduces additional control to an existing diffusion model by copying only the downsample layer weights of the diffusion U-Net. The original weights are frozen and the copied weights are connected to the original model via a “zero convolution” <strong>in the upsample part</strong> of the U-Net.</p> <p>The “zero convolution” is simply a convolution with all weights set to zero. This way, the original model is not affected by the copied weights at the beginning of training. The zero weights become non-zero while training ControlNet.</p> <p>Before passing conditioning information through the copied layers, it is encoded by an additional encoder model.</p> <p><img src="/assets/img/blog/controlling_diffusion/controlnet.png" alt="ControlNet Architecture" style="width: 100%; height: auto;"/></p> <ul> <li>increases memory size (23%) and time (34%) for training</li> <li>not lightweight, but not as heavy as cross-attention or concatenation on conditioning information</li> </ul> <h2 id="t2i-adapter"><a href="https://github.com/TencentARC/T2I-Adapter">T2I-Adapter</a></h2> <p>T2I-Adapter is more lightweight than ControlNet. It does not finetune any of the original weights. Instead, it <strong>only</strong> injects conditioning information to the encoder/downsample layers of the diffusion U-Net by <strong>addition</strong>.</p> <p>The conditioning (512x512) is brought to the size of the first encoder layer (64x64) by <a href="https://pytorch.org/docs/stable/generated/torch.nn.PixelUnshuffle.html">pixel unshuffling</a> which seems to move some pixel data of an image to the channel dimension. Then it is encoded by a small multilayer convolutional network to the size of each U-Net layer.</p> <p>The biggest advantage of T2I over ControlNet I see is composability. You can use multiple conditionings by simply adding them together.</p> <p>An interesting application of T2I-Adapter in the paper is sequential editing where a T2I-Adapter is applied to an image multiple times.</p> <p><img src="/assets/img/blog/controlling_diffusion/sequential_edit_t2i_adapter.png" alt="T2I-Adapter Architecture" style="width: 100%; height: auto;"/></p> <ul> <li>lightweight (300MB)</li> <li>composability</li> </ul> <h1 id="no-training-required">No Training Required</h1> <h2 id="gradient-based-guidance---clip-guidance">Gradient-Based Guidance -&gt; CLIP Guidance</h2> <p>This method uses the gradients of a distance function to steer image generation.</p> <p>Concretely: CLIP encodes both text and image into a shared vector space. In CLIP guidance, a difference between the generated image and the text prompt is calculated. From this difference, gradients are calculated and added to the current noise prediction.</p> <p>Simplified code from <a href="https://github.com/huggingface/diffusers/blob/v0.13.1-patch/examples/community/clip_guided_stable_diffusion.py#L167">diffusers</a>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span> <span class="o">=</span> <span class="nf">spherical_dist_loss</span><span class="p">(</span><span class="n">image_embeddings_clip</span><span class="p">,</span> <span class="n">text_embeddings_clip</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span> <span class="o">*</span> <span class="n">clip_guidance_scale</span>
<span class="n">grads</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">latents</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">noise_pred</span> <span class="o">=</span> <span class="n">noise_pred_original</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">beta_prod_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">grads</span>
</code></pre></div></div> <p>I have used this approach together with a face recognition model to steer generation towards generating faces that look like a groundtruth face. The generated face was very similar to the ground truth face but not perfect. By itself, it is probably not enough. The authors of <a href="https://arxiv.org/abs/2212.06512">DifFace</a> use this method combined with a learned face conditioning to improve the generation result.</p> <p>This is generated me: <img src="/assets/img/blog/controlling_diffusion/target_philipp.png" alt="Generated me" style="width: 100%; height: auto;"/></p> <ul> <li>no increase in model size</li> <li>no retraining</li> <li>you could apply multiple conditionings this way</li> </ul> <h3 id="other-applications">Other applications:</h3> <ul> <li>As far as I know, this has also been used to enforce depth in generated images by using a MiDaS model.</li> <li>It has also been used to <a href="https://hanhung.github.io/PureCLIPNeRF/">generate 3D objects without any training data</a>!</li> </ul> <h2 id="partial-diffusion-inpaintingoutpainting">Partial Diffusion (Inpainting/Outpainting)</h2> <p>This method is limited to inpainting and outpainting based on existing image areas. It works by only using random noise for the new areas to generate and using noised original image areas for the areas to keep.</p> <p>Example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">noisy_guidance_latents</span> <span class="o">=</span> <span class="n">scheduler</span><span class="p">.</span><span class="nf">add_noise</span><span class="p">(</span><span class="n">original_image_latents</span><span class="p">,</span> <span class="n">guidance_noise</span><span class="p">,</span> <span class="n">timestep</span><span class="p">)</span>
<span class="n">latent_model_input</span> <span class="o">=</span> <span class="n">latent_model_input</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mask</span><span class="p">)</span> <span class="o">+</span> <span class="n">noisy_guidance_latents</span> <span class="o">*</span> <span class="n">mask</span>
<span class="n">noise_pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">unet</span><span class="p">(</span><span class="n">latent_model_input</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">text_embeddings</span><span class="p">,).</span><span class="n">sample</span>
</code></pre></div></div> <p>This method can also be used for video diffusion to generate new frames based on previous ones. The advantage is that retraining is not necessary.</p> <h1 id="final-thoughts">Final Thoughts</h1> <p>It seems like there is a push towards a decoupling of conditioning and generative model. Although in Stable Diffusion the text conditioning was still trained jointly via cross-attention, I could imagine future image generative models train that conditioning separately. This would make it possible to switch text encoders without retraining the whole diffusion model and the diffusion model would be more lightweight with cross-attention being removed.</p>]]></content><author><name></name></author><category term="diffusion"/><category term="control"/><category term="stablediffusion"/><summary type="html"><![CDATA[How can I steer generative models like Stable Diffusion?]]></summary></entry></feed>