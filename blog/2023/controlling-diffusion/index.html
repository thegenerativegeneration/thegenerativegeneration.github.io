<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Control - How to Steer Diffusion Models | Philipp Haslbauer </title> <meta name="author" content="Philipp Haslbauer"> <meta name="description" content="How can I steer generative models like Stable Diffusion?"> <meta name="keywords" content="portfolio"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|VT323:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?939c8e259b24315d07802d7802e073bd"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://thegenerativegeneration.github.io/blog/2023/controlling-diffusion/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Philipp Haslbauer </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/works/">works </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/experiments/">experiments </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Control - How to Steer Diffusion Models</h1> <p class="post-meta"> Created on March 05, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/diffusion"> <i class="fa-solid fa-tag fa-sm"></i> diffusion</a>   <a href="/blog/category/control"> <i class="fa-solid fa-tag fa-sm"></i> control</a>   <a href="/blog/category/stablediffusion"> <i class="fa-solid fa-tag fa-sm"></i> stablediffusion</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>On this page, I have compiled several methods to steer the diffusion process. If not otherwise stated, I present the methods as they are used with Stable Diffusion.</p> <p>Although the mechanisms presented are used in 2D image generation, I assume that they can be used in the generation of other modalities.</p> <p>I will try to keep this article up to date. If you have any suggestions, please let me know.</p> <h1 id="finetune">Finetune</h1> <h2 id="cross-attention">Cross Attention</h2> <p>In Stable Diffusion (and Imagen/Dalle 2) text prompts are used to condition the image generation. The text prompts are encoded by a text model. The encoding is passed into the diffusion model to a cross-attention layer. The cross-attention layer can also be used to condition on different information. However, adding new conditioning requires retraining.</p> <p>This is probably not great when you want to combine multiple conditionings as each would likely require its own cross-attention layers.</p> <ul> <li>increases model size</li> <li>adds cross-attention layers</li> <li>requires retraining or finetuning</li> </ul> <h2 id="concatenation">Concatenation</h2> <p>If the conditioning you want to use has the same size as the diffusion model input, you can use concatenation. If it doesn’t, you can still embed it into this shape.</p> <p>For example, you can train an upsampling model by concatenating (in the channel dimension) the noisy input and a lower-resolution input that is bilinearly upsampled to the input size. For this specific use case, the input size of the model increases from <code class="language-plaintext highlighter-rouge">BxCxHxW</code> to <code class="language-plaintext highlighter-rouge">Bx2*CxHxW</code> (“Guided-Diffusion Paper”).</p> <p>Some video diffusion papers applied this method to interpolate frames in the frame/time dimension.</p> <ul> <li>increases input size in the channel dimension</li> <li>requires retraining or finetuning</li> <li>can be an alternative to cross-attention</li> </ul> <h1 id="train-a-new-model">Train a New Model</h1> <h2 id="controlnet"><a href="https://github.com/lllyasviel/ControlNet" rel="external nofollow noopener" target="_blank">ControlNet</a></h2> <p>ControlNet introduces additional control to an existing diffusion model by copying only the downsample layer weights of the diffusion U-Net. The original weights are frozen and the copied weights are connected to the original model via a “zero convolution” <strong>in the upsample part</strong> of the U-Net.</p> <p>The “zero convolution” is simply a convolution with all weights set to zero. This way, the original model is not affected by the copied weights at the beginning of training. The zero weights become non-zero while training ControlNet.</p> <p>Before passing conditioning information through the copied layers, it is encoded by an additional encoder model.</p> <p><img src="/assets/img/blog/controlling_diffusion/controlnet.png" alt="ControlNet Architecture" style="width: 100%; height: auto;"></p> <ul> <li>increases memory size (23%) and time (34%) for training</li> <li>not lightweight, but not as heavy as cross-attention or concatenation on conditioning information</li> </ul> <h2 id="t2i-adapter"><a href="https://github.com/TencentARC/T2I-Adapter" rel="external nofollow noopener" target="_blank">T2I-Adapter</a></h2> <p>T2I-Adapter is more lightweight than ControlNet. It does not finetune any of the original weights. Instead, it <strong>only</strong> injects conditioning information to the encoder/downsample layers of the diffusion U-Net by <strong>addition</strong>.</p> <p>The conditioning (512x512) is brought to the size of the first encoder layer (64x64) by <a href="https://pytorch.org/docs/stable/generated/torch.nn.PixelUnshuffle.html" rel="external nofollow noopener" target="_blank">pixel unshuffling</a> which seems to move some pixel data of an image to the channel dimension. Then it is encoded by a small multilayer convolutional network to the size of each U-Net layer.</p> <p>The biggest advantage of T2I over ControlNet I see is composability. You can use multiple conditionings by simply adding them together.</p> <p>An interesting application of T2I-Adapter in the paper is sequential editing where a T2I-Adapter is applied to an image multiple times.</p> <p><img src="/assets/img/blog/controlling_diffusion/sequential_edit_t2i_adapter.png" alt="T2I-Adapter Architecture" style="width: 100%; height: auto;"></p> <ul> <li>lightweight (300MB)</li> <li>composability</li> </ul> <h1 id="no-training-required">No Training Required</h1> <h2 id="gradient-based-guidance---clip-guidance">Gradient-Based Guidance -&gt; CLIP Guidance</h2> <p>This method uses the gradients of a distance function to steer image generation.</p> <p>Concretely: CLIP encodes both text and image into a shared vector space. In CLIP guidance, a difference between the generated image and the text prompt is calculated. From this difference, gradients are calculated and added to the current noise prediction.</p> <p>Simplified code from <a href="https://github.com/huggingface/diffusers/blob/v0.13.1-patch/examples/community/clip_guided_stable_diffusion.py#L167" rel="external nofollow noopener" target="_blank">diffusers</a>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span> <span class="o">=</span> <span class="nf">spherical_dist_loss</span><span class="p">(</span><span class="n">image_embeddings_clip</span><span class="p">,</span> <span class="n">text_embeddings_clip</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span> <span class="o">*</span> <span class="n">clip_guidance_scale</span>
<span class="n">grads</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">latents</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">noise_pred</span> <span class="o">=</span> <span class="n">noise_pred_original</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">beta_prod_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">grads</span>
</code></pre></div></div> <p>I have used this approach together with a face recognition model to steer generation towards generating faces that look like a groundtruth face. The generated face was very similar to the ground truth face but not perfect. By itself, it is probably not enough. The authors of <a href="https://arxiv.org/abs/2212.06512" rel="external nofollow noopener" target="_blank">DifFace</a> use this method combined with a learned face conditioning to improve the generation result.</p> <p>This is generated me: <img src="/assets/img/blog/controlling_diffusion/target_philipp.png" alt="Generated me" style="width: 100%; height: auto;"></p> <ul> <li>no increase in model size</li> <li>no retraining</li> <li>you could apply multiple conditionings this way</li> </ul> <h3 id="other-applications">Other applications:</h3> <ul> <li>As far as I know, this has also been used to enforce depth in generated images by using a MiDaS model.</li> <li>It has also been used to <a href="https://hanhung.github.io/PureCLIPNeRF/" rel="external nofollow noopener" target="_blank">generate 3D objects without any training data</a>!</li> </ul> <h2 id="partial-diffusion-inpaintingoutpainting">Partial Diffusion (Inpainting/Outpainting)</h2> <p>This method is limited to inpainting and outpainting based on existing image areas. It works by only using random noise for the new areas to generate and using noised original image areas for the areas to keep.</p> <p>Example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">noisy_guidance_latents</span> <span class="o">=</span> <span class="n">scheduler</span><span class="p">.</span><span class="nf">add_noise</span><span class="p">(</span><span class="n">original_image_latents</span><span class="p">,</span> <span class="n">guidance_noise</span><span class="p">,</span> <span class="n">timestep</span><span class="p">)</span>
<span class="n">latent_model_input</span> <span class="o">=</span> <span class="n">latent_model_input</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mask</span><span class="p">)</span> <span class="o">+</span> <span class="n">noisy_guidance_latents</span> <span class="o">*</span> <span class="n">mask</span>
<span class="n">noise_pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">unet</span><span class="p">(</span><span class="n">latent_model_input</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">text_embeddings</span><span class="p">,).</span><span class="n">sample</span>
</code></pre></div></div> <p>This method can also be used for video diffusion to generate new frames based on previous ones. The advantage is that retraining is not necessary.</p> <h1 id="final-thoughts">Final Thoughts</h1> <p>It seems like there is a push towards a decoupling of conditioning and generative model. Although in Stable Diffusion the text conditioning was still trained jointly via cross-attention, I could imagine future image generative models train that conditioning separately. This would make it possible to switch text encoders without retraining the whole diffusion model and the diffusion model would be more lightweight with cross-attention being removed.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://philipphaslbauer.substack.com/p/coming-soon" target="_blank" rel="external nofollow noopener">Coming soon</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/easy-3dgs/">Easiest Way To Use 3D Gaussians Splatting</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Philipp Haslbauer. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>